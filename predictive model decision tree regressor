import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.feature_selection import SelectKBest, chi2,f_classif

filename = '/unmet demand data.csv'
df = pd.read_csv(filename,header=0,dtype = {'Zip code':str,'month':str,'product code':str})

y = df[['Unmet demand']]
#y.info()
x = df.drop(['Unmet demand'], axis= 1)
#x.info()

#x = x.apply(pd.to_numeric, errors='coerce')
#y = y.apply(pd.to_numeric, errors='coerce')
#x.fillna(0, inplace=True)
#y.fillna(0, inplace=True)

x = pd.get_dummies(x)


best_f = SelectKBest(score_func=f_classif, k=418)
fit = best_f.fit(x,y)
dfscore = pd.DataFrame(fit.scores_)
dfcol = pd.DataFrame(x.columns)
featurescore = pd.concat([dfcol,dfscore],axis=1)
featurescore.columns = ['Features','Score']
f_list = featurescore.nsmallest(368,'Score')
#print(f_list)
f_list2 = f_list["Features"]
#print(f_list2.head(10))
x_f = x.drop(f_list2, axis= 1)
#x_f.info()

x_train, x_test, y_train, y_test = train_test_split(x_f,y,test_size=0.2)

model= DecisionTreeRegressor()
fit= model.fit(x_train,y_train)
y_pred = fit.predict(x_test)
#score = model.score(x_test,y_test)
#print(score)
mse = mean_squared_error(y_test,y_pred)
#print(score)
#print(mse)
RMSE = np.sqrt(mse)
print(RMSE)
